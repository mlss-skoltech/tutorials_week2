{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q --upgrade git+https://github.com/mlss-skoltech/tutorials_week2.git#subdirectory=graph_neural_networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /anaconda3/lib/python3.7/site-packages/gnnutils/data/data.zip\n",
      "   creating: ./data/\n",
      "  inflating: ./data/ind.cora.ally    \n",
      "   creating: ./__MACOSX/\n",
      "   creating: ./__MACOSX/data/\n",
      "  inflating: ./__MACOSX/data/._ind.cora.ally  \n",
      "  inflating: ./data/ind.cora.test.index  \n",
      "  inflating: ./__MACOSX/data/._ind.cora.test.index  \n",
      "  inflating: ./data/ind.pubmed.graph  \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.graph  \n",
      "  inflating: ./data/ind.cora.allx    \n",
      "  inflating: ./__MACOSX/data/._ind.cora.allx  \n",
      "  inflating: ./data/ind.citeseer.test.index  \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.test.index  \n",
      "  inflating: ./data/ind.citeseer.ty  \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.ty  \n",
      "  inflating: ./data/ind.citeseer.tx  \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.tx  \n",
      "  inflating: ./data/ind.citeseer.graph  \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.graph  \n",
      "  inflating: ./data/preprocessed_MNIST.dump  \n",
      "  inflating: ./__MACOSX/data/._preprocessed_MNIST.dump  \n",
      "  inflating: ./data/ind.cora.tx      \n",
      "  inflating: ./__MACOSX/data/._ind.cora.tx  \n",
      "  inflating: ./data/ind.pubmed.test.index  \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.test.index  \n",
      "  inflating: ./data/ind.cora.ty      \n",
      "  inflating: ./__MACOSX/data/._ind.cora.ty  \n",
      "  inflating: ./data/ind.cora.graph   \n",
      "  inflating: ./__MACOSX/data/._ind.cora.graph  \n",
      "  inflating: ./data/ind.citeseer.y   \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.y  \n",
      "  inflating: ./data/ind.citeseer.ally  \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.ally  \n",
      "  inflating: ./data/ind.citeseer.allx  \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.allx  \n",
      "  inflating: ./data/ind.citeseer.x   \n",
      "  inflating: ./__MACOSX/data/._ind.citeseer.x  \n",
      "  inflating: ./data/ind.pubmed.ally  \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.ally  \n",
      "  inflating: ./data/ind.pubmed.allx  \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.allx  \n",
      "  inflating: ./data/ind.pubmed.x     \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.x  \n",
      "  inflating: ./data/ind.pubmed.tx    \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.tx  \n",
      "  inflating: ./data/ind.cora.x       \n",
      "  inflating: ./__MACOSX/data/._ind.cora.x  \n",
      "  inflating: ./data/ind.pubmed.ty    \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.ty  \n",
      "  inflating: ./data/ind.pubmed.y     \n",
      "  inflating: ./__MACOSX/data/._ind.pubmed.y  \n",
      "  inflating: ./data/ind.cora.y       \n",
      "  inflating: ./__MACOSX/data/._ind.cora.y  \n",
      "  inflating: ./__MACOSX/._data       \n"
     ]
    }
   ],
   "source": [
    "import pkg_resources\n",
    "\n",
    "ZIP_PATH = pkg_resources.resource_filename('gnnutils', 'data/data.zip')\n",
    "DATA_PATH = './data'\n",
    "\n",
    "!unzip -u {ZIP_PATH} -d ./"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time, shutil\n",
    "import numpy as np\n",
    "import os, collections, sklearn\n",
    "import joblib\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â For simplicity we do the entire pre-processing a priori and we just load the data here\n",
    "res = joblib.load(DATA_PATH + '/preprocessed_MNIST.dump')\n",
    "A, laplacians, normalized_laplacians, train_data, val_data, test_data, train_labels, val_labels, test_labels = res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CayleyNet:\n",
    "    \"\"\"\n",
    "    The neural network model.\n",
    "    \"\"\"\n",
    "    \n",
    "    #Helper functions used for constructing the model\n",
    "    def _weight_variable(self, shape, regularization=True, name=\"\"): \n",
    "        \"\"\"Initializer for the weights\"\"\"\n",
    "        \n",
    "        initial = tf.truncated_normal_initializer(0, 0.1)\n",
    "        var = tf.get_variable('weights'+name, shape, tf.float32, initializer=initial)\n",
    "        if regularization: #append the loss of the current variable to the regularization term \n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "    def _bias_variable(self, shape, regularization=True):\n",
    "        \"\"\"Initializer for the bias\"\"\"\n",
    "        \n",
    "        initial = tf.constant_initializer(0.1)\n",
    "        var = tf.get_variable('bias', shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "    \n",
    "    def _h_variable(self, shape, regularization=False, name=''):\n",
    "        \"\"\"Initializer for the zoom parameter h\"\"\"\n",
    "        \n",
    "        initial = tf.random_uniform_initializer()\n",
    "        var = tf.get_variable('h'+name, shape, tf.float32, initializer=initial)\n",
    "        if regularization:\n",
    "            self.regularizers.append(tf.nn.l2_loss(var))\n",
    "        return var\n",
    "\n",
    "    def frobenius_norm(self, tensor): \n",
    "        \"\"\"Computes the frobenius norm for a given laplacian\"\"\"\n",
    "        \n",
    "        square_tensor = tf.square(tensor)\n",
    "        tensor_sum = tf.reduce_sum(square_tensor)\n",
    "        frobenius_norm = tf.sqrt(tensor_sum)\n",
    "        return frobenius_norm\n",
    "    \n",
    "    def compute_sparse_D_inv_indices(self, M):\n",
    "        \"\"\"Computes the indices required for constructing a sparse version of D^-1.\"\"\"\n",
    "        \n",
    "        idx_main_diag = np.tile(np.expand_dims(np.arange(0, 2*M),1), [1, 2])\n",
    "        idx_diag_ur = np.concatenate([np.expand_dims(np.arange(0, M),1), np.expand_dims(np.arange(0, M)+M,1)], 1)\n",
    "        idx_diag_ll = np.concatenate([np.expand_dims(np.arange(0, M)+M,1), np.expand_dims(np.arange(0, M),1)], 1)\n",
    "        idx = np.concatenate([idx_main_diag, idx_diag_ur, idx_diag_ll], 0)\n",
    "        return idx  \n",
    "    \n",
    "    def compute_sparse_R_indices(self, L_off_diag, M):\n",
    "        \"\"\"Computes the indices required for constructing a sparse version of R.\"\"\"\n",
    "        \n",
    "        idx_L = np.asarray(np.where(L_off_diag)).T\n",
    "        idx_L_sh = idx_L + np.expand_dims(np.asarray([M,M]),0)\n",
    "        idx = np.concatenate([idx_L, idx_L_sh])\n",
    "        return idx\n",
    "    \n",
    "    def compute_sparse_numerator_projection_indices(self, L, M):\n",
    "        \"\"\"Computes the indices required for constructing the numerator projection sparse matrix.\"\"\"\n",
    "        \n",
    "        idx_L = np.asarray(np.where(L)).T\n",
    "        idx_L_sh = idx_L + np.expand_dims(np.asarray([M,M]),0)\n",
    "        idx_diag_ur = np.concatenate([np.expand_dims(np.arange(0, M),1), np.expand_dims(np.arange(0, M)+M,1)], 1)\n",
    "        idx_diag_ll = np.concatenate([np.expand_dims(np.arange(0, M)+M,1), np.expand_dims(np.arange(0, M),1)], 1)\n",
    "        idx = np.concatenate([idx_L, idx_L_sh, idx_diag_ur, idx_diag_ll])\n",
    "        return idx\n",
    "    \n",
    "    def cayleyConv(self, x, L_np, Fout, K): \n",
    "        \"\"\"Applies chebyshev polynomials over the graph.\"\"\"\n",
    "        \n",
    "        M, Fin = x.get_shape()[1:] # M the number of samples in the images, Fin the number of features\n",
    "        M, Fin = int(M), int(Fin)\n",
    "        N = tf.shape(x)[0] # N is the number of images\n",
    "        \n",
    "        # Applies cayley transform by means of Jacobi method.\n",
    "        diag_L_np = np.diag(L_np)  # vector containing the diagonal of L\n",
    "        L_off_diag_np = L_np - np.diag(diag_L_np) # off-diagonal entries of L \n",
    "        \n",
    "        list_x_pos_exp = [tf.cast(tf.expand_dims(x,0), 'complex64')] # 1 x N x M x F\n",
    "        \n",
    "        for iii in range(self.n_h):  # for every zoom parameter we want to use (typically one).\n",
    "            h = self._h_variable([1,1], regularization=False, name='_h%f' % iii)\n",
    "            self.list_h.append(h)\n",
    "            \n",
    "            # Computes matrices required by Jacobi (https://en.wikipedia.org/wiki/Jacobi_method)\n",
    "            \n",
    "            # To make things more efficient we reprent a complex vector of shape M as real vector of shape 2*M\n",
    "            # where the first M values represent real coefficients while the second M the imaginary ones.\n",
    "            # All the matrices here defined are computed according to such notation (it allows to use sparse matrices\n",
    "            # with TF with complex values).\n",
    "            \n",
    "            # ************************** COMPUTES numerator projection **************************\n",
    "            idx = self.compute_sparse_numerator_projection_indices(L_np, M)\n",
    "            \n",
    "            vals_L = tf.squeeze(h*L_np[np.where(L_np)])\n",
    "            vals = tf.concat([vals_L, vals_L, tf.ones([M,]), -tf.ones([M,])], 0)\n",
    "            \n",
    "            cayley_op_neg_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n",
    "            cayley_op_neg_sp = tf.sparse_reorder(cayley_op_neg_sp)\n",
    "        \n",
    "            # ************************** COMPUTES D **************************\n",
    "            D_real = tf.squeeze(h*diag_L_np)\n",
    "            D = tf.complex(D_real, tf.ones_like(D_real))\n",
    "            D_inv = tf.pow(D, -tf.ones_like(D)) # vector of M elements <- diagonal of D^-1\n",
    "            \n",
    "            idx = self.compute_sparse_D_inv_indices(M)\n",
    "            vals = tf.concat([tf.real(D_inv), tf.real(D_inv), -tf.imag(D_inv), tf.imag(D_inv)], 0)\n",
    "            \n",
    "            D_inv_ext_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n",
    "            D_inv_ext_sp = tf.sparse_reorder(D_inv_ext_sp)\n",
    "            \n",
    "            # ************************** COMPUTES R **************************\n",
    "            idx = self.compute_sparse_R_indices(L_off_diag_np, M)\n",
    "            \n",
    "            vals_L = tf.squeeze(h*L_off_diag_np[np.where(L_off_diag_np)])\n",
    "            vals = tf.concat([vals_L, vals_L], 0)\n",
    "            \n",
    "            R_sp = tf.SparseTensor(idx, vals, [M*2, M*2])\n",
    "            R_sp = tf.sparse_reorder(R_sp)\n",
    "            \n",
    "            # Applies Jacobi method\n",
    "            c_transform = tf.transpose(x, [1,0,2]) # shape = M, N, F\n",
    "            c_transform = tf.reshape(c_transform, [M, -1]) # shape = M, N*F\n",
    "            last_sol = tf.concat([c_transform, tf.zeros_like(c_transform)],0)\n",
    "            for k in range(K):  # for every order of our polynomial\n",
    "                \n",
    "                # Jacobi initialization\n",
    "                b = tf.sparse_tensor_dense_matmul(cayley_op_neg_sp, last_sol) # shape = M, N*F\n",
    "                a = tf.sparse_tensor_dense_matmul(D_inv_ext_sp, b) # shape = M, N*F\n",
    "                \n",
    "                # Jacobi iterations\n",
    "                cond = lambda i, _: tf.less(i, self.num_jacobi_iter)\n",
    "                body = lambda i, c_sol: [tf.add(i, 1), a  - tf.sparse_tensor_dense_matmul(D_inv_ext_sp, \n",
    "                                                                                          tf.sparse_tensor_dense_matmul(R_sp, c_sol))]\n",
    "                \n",
    "                c_sol = tf.while_loop(cond, body, [0, a], parallel_iterations=1, swap_memory=True)\n",
    "                c_sol = c_sol[-1]\n",
    "                    \n",
    "                # Constructs and saves the final complex matrices\n",
    "                c_sol_complex = tf.complex(c_sol[:M,:], c_sol[M:, :]) #M x N*F\n",
    "                c_sol_reshaped = tf.reshape(c_sol_complex, [M, -1, Fin])\n",
    "                c_sol_reshaped = tf.transpose(c_sol_reshaped, [1, 0, 2]) #N x M x F\n",
    "                list_x_pos_exp.append(tf.expand_dims(c_sol_reshaped,0)) #1 x N x M x Flist_x_pos_exp\n",
    "                \n",
    "                last_sol = c_sol\n",
    "        x_pos_exp = tf.concat(list_x_pos_exp, 0) # shape = n_h*K x N x M x Fin\n",
    "        x_pos_exp = tf.transpose(x_pos_exp, [1,2,0,3])  #N x M x n_h*K x Fin\n",
    "        x_pos_exp = tf.reshape(x_pos_exp, [N*M, -1]) #N*M x 2*K*Fin\n",
    "        \n",
    "        real_conv_weights = self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_real')\n",
    "        imag_conv_weights = self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')#tf.ones([Fin*(self.n_h*K+1), Fout])#self._weight_variable([Fin*(self.n_h*K+1), Fout], regularization=False, name='_imag')\n",
    "        \n",
    "        W_pos_exp = tf.complex(real_conv_weights, -imag_conv_weights)\n",
    "        \n",
    "        x_pos_exp_filt = tf.matmul(x_pos_exp, W_pos_exp)\n",
    "        \n",
    "        x_filt = 2*tf.real(x_pos_exp_filt)\n",
    "        return tf.reshape(x_filt, [N, M, Fout])\n",
    "\n",
    "\n",
    "    def b1relu(self, x): #sums a bias and applies relu\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n",
    "\n",
    "\n",
    "    def mpool1(self, x, p): #efficient pooling realized thanks to the reordering of the laplacians we have done a priori\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "    \n",
    "\n",
    "    def b1relu(self, x): #sums a bias and applies relu\n",
    "        \"\"\"Bias and ReLU. One bias per filter.\"\"\"\n",
    "        N, M, F = x.get_shape()\n",
    "        b = self._bias_variable([1, 1, int(F)], regularization=False)\n",
    "        return tf.nn.relu(x + b) #add the bias to the convolutive layer\n",
    "\n",
    "\n",
    "    def mpool1(self, x, p): #efficient pooling realized thanks to the reordering of the laplacians we have done a priori\n",
    "        \"\"\"Max pooling of size p. Should be a power of 2.\"\"\"\n",
    "        if p > 1:\n",
    "            x = tf.expand_dims(x, 3)  # N x M x F x 1\n",
    "            x = tf.nn.max_pool(x, ksize=[1,p,1,1], strides=[1,p,1,1], padding='SAME')\n",
    "            return tf.squeeze(x, [3])  # N x M/p x F\n",
    "        else:\n",
    "            return x\n",
    "\n",
    "    def fc(self, x, Mout, relu=True):\n",
    "        \"\"\"Fully connected layer with Mout features.\"\"\"\n",
    "        N, Min = x.get_shape()\n",
    "        W = self._weight_variable([int(Min), Mout], regularization=True)\n",
    "        b = self._bias_variable([Mout], regularization=True)\n",
    "        x = tf.matmul(x, W) + b\n",
    "        return tf.nn.relu(x) if relu else x\n",
    "    \n",
    "    #function used for extracting the result of our model\n",
    "    def _inference(self, x, dropout): #definition of the model\n",
    "        \n",
    "        # Graph convolutional layers.\n",
    "        x = tf.expand_dims(x, 2)  # N x M x F=1\n",
    "        j = 0\n",
    "        self.list_h = list()\n",
    "        for i in range(len(self.p)):\n",
    "            with tf.variable_scope('cgconv{}'.format(i+1)):\n",
    "                with tf.name_scope('filter'):\n",
    "                    x = self.cayleyConv(x, self.L_np[i*2], self.F[i], self.K[i])\n",
    "                    if (i==0):\n",
    "                        self.debug = x\n",
    "                with tf.name_scope('bias_relu'):\n",
    "                    x = self.b1relu(tf.cast(tf.real(x), 'float32'))\n",
    "                with tf.name_scope('pooling'):\n",
    "                    x = self.mpool1(x, self.p[i])\n",
    "                    \n",
    "            j += int(np.log2(self.p[i])) if self.p[i] > 1 else 0\n",
    "        \n",
    "        # Fully connected hidden layers.\n",
    "        _, M, F = x.get_shape()\n",
    "        x = tf.reshape(x, [-1, int(M*F)])  # N x M\n",
    "        for i,M in enumerate(self.M[:-1]): #apply a fully connected layer for each layer defined in M\n",
    "                                           #(we discard the last value in M since it contains the number of classes we have\n",
    "                                           #to predict)\n",
    "            with tf.variable_scope('fc{}'.format(i+1)):\n",
    "                x = self.fc(x, M)\n",
    "                x = tf.nn.dropout(x, dropout)\n",
    "        \n",
    "        # Logits linear layer, i.e. softmax without normalization.\n",
    "        with tf.variable_scope('logits'):\n",
    "            x = self.fc(x, self.M[-1], relu=False)\n",
    "        return x\n",
    "    \n",
    "    def __init__(self, p, K, F, M, M_0, batch_size, num_jacobi_iter, L,\n",
    "                 decay_steps, decay_rate, learning_rate=1e-4, momentum=0.9, regularization=5e-4, clip_norm=1e10,\n",
    "                 idx_gpu = '/gpu:0'):\n",
    "        self.regularizers = list() #list of regularization l2 loss for multiple variables\n",
    "        self.n_h = 1\n",
    "        self.num_jacobi_iter = num_jacobi_iter\n",
    "        self.p = p #dimensions of the pooling layers\n",
    "        self.K = K #List of polynomial orders, i.e. filter sizes or number of hops\n",
    "        self.F = F #Number of features of convolutional layers\n",
    "        \n",
    "        self.M = M #Number of neurons in fully connected layers\n",
    "        \n",
    "        self.M_0 = M_0 #number of elements in the first graph \n",
    "        \n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        #definition of some learning parameters\n",
    "        self.decay_steps = decay_steps\n",
    "        self.decay_rate = decay_rate\n",
    "        self.learning_rate = learning_rate\n",
    "        self.regularization = regularization\n",
    "        \n",
    "        with tf.Graph().as_default() as g:\n",
    "                self.graph = g\n",
    "                tf.set_random_seed(0)\n",
    "                with tf.device(idx_gpu):\n",
    "                        #definition of placeholders\n",
    "                        self.L_np = [c_L.toarray().astype('float32') for c_L in L]\n",
    "                        self.ph_data = tf.placeholder(tf.float32, (self.batch_size, M_0), 'data')\n",
    "                        self.ph_labels = tf.placeholder(tf.int32, (self.batch_size), 'labels')\n",
    "                        self.ph_dropout = tf.placeholder(tf.float32, (), 'dropout')\n",
    "                    \n",
    "                        #Model construction\n",
    "                        self.logits = self._inference(self.ph_data, self.ph_dropout)\n",
    "                        \n",
    "                        #Definition of the loss function\n",
    "                        with tf.name_scope('loss'):\n",
    "                            self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(logits=self.logits, labels=self.ph_labels)\n",
    "                            self.cross_entropy = tf.reduce_mean(self.cross_entropy)\n",
    "                        with tf.name_scope('regularization'):\n",
    "                            self.regularization *= tf.add_n(self.regularizers)\n",
    "                        self.loss = self.cross_entropy + self.regularization\n",
    "                        \n",
    "                        #Solver Definition\n",
    "                        with tf.name_scope('training'):\n",
    "                            # Learning rate.\n",
    "                            global_step = tf.Variable(0, name='global_step', trainable=False) #used for counting how many iterations we have done\n",
    "                            if decay_rate != 1: #applies an exponential decay of the lr wrt the number of iterations done\n",
    "                                learning_rate = tf.train.exponential_decay(\n",
    "                                        learning_rate, global_step, decay_steps, decay_rate, staircase=True)\n",
    "                            # Optimizer.\n",
    "                            if momentum == 0:\n",
    "                                optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "                            else: #applies momentum for increasing the robustness of the gradient \n",
    "                                optimizer = tf.train.MomentumOptimizer(learning_rate, momentum)\n",
    "                            #grads = optimizer.compute_gradients(self.loss)\n",
    "                            tvars = tf.trainable_variables()\n",
    "                            #grads, _ = tf.clip_by_global_norm(tf.gradients(self.loss, tvars), clip_norm)\n",
    "                            grads, variables = zip(*optimizer.compute_gradients(self.loss))\n",
    "                            grads, _ = tf.clip_by_global_norm(grads, clip_norm)\n",
    "                            self.op_gradients = optimizer.apply_gradients(zip(grads, variables), \n",
    "                                                                          global_step=global_step)\n",
    "                            \n",
    "                        #Computation of the norm gradients (useful for debugging)\n",
    "                        self.var_grad = tf.gradients(self.loss, tf.trainable_variables())\n",
    "                        self.norm_grad = self.frobenius_norm(tf.concat([tf.reshape(g, [-1]) for g in self.var_grad], 0))\n",
    "\n",
    "                        #Extraction of the predictions and computation of accuracy\n",
    "                        self.predictions = tf.cast(tf.argmax(self.logits, dimension=1), tf.int32)\n",
    "                        self.accuracy = 100 * tf.contrib.metrics.accuracy(self.predictions, self.ph_labels)\n",
    "        \n",
    "                        # Create a session for running Ops on the Graph.\n",
    "                        config = tf.ConfigProto(allow_soft_placement = True)\n",
    "                        config.gpu_options.allow_growth = True\n",
    "                        self.session = tf.Session(config=config)\n",
    "\n",
    "                        # Run the Op to initialize the variables.\n",
    "                        init = tf.global_variables_initializer()\n",
    "                        self.session.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convolutional parameters\n",
    "p = [4, 4]   # Dimensions of the pooling layers\n",
    "K = [12, 12] # List of polynomial orders, i.e. filter sizes or number of hops\n",
    "F = [32, 64] # Number of features of convolutional layers\n",
    "\n",
    "#FC parameters\n",
    "C = max(train_labels) + 1 # Number of classes we have\n",
    "M = [512, C] # Number of neurons in fully connected layers\n",
    "\n",
    "#Solver parameters\n",
    "batch_size = 100\n",
    "decay_steps = train_data.shape[0] / batch_size # number of steps to do before decreasing the learning rate\n",
    "decay_rate = 0.95\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "regularization = 5e-4\n",
    "\n",
    "# Definition of keep probabilities for dropout layers\n",
    "dropout_training = 0.5\n",
    "dropout_val_test = 1.0\n",
    "\n",
    "num_jacobi_iter = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From <ipython-input-8-a8adee33db31>:231: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-8-a8adee33db31>:306: calling argmax (from tensorflow.python.ops.math_ops) with dimension is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the `axis` argument instead\n",
      "\n",
      "WARNING: The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Construction of the learning obj\n",
    "M_0 = normalized_laplacians[0].shape[0] # number of elements in the first graph\n",
    "learning_obj = CayleyNet(p, K, F, M, M_0, batch_size, num_jacobi_iter, normalized_laplacians,\n",
    "                         decay_steps, decay_rate,\n",
    "                         learning_rate=learning_rate, regularization=regularization,\n",
    "                         momentum=momentum)#, clip_norm=100)\n",
    "\n",
    "# definition of overall number of training iterations and validation frequency\n",
    "num_iter_val = 600\n",
    "num_total_iter_training = 21000\n",
    "\n",
    "num_iter = 0\n",
    "\n",
    "list_training_loss = list()\n",
    "list_training_norm_grad = list()\n",
    "list_val_accuracy = list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TRN] iter = 000, cost = 2.10e+01, |grad| = 9.11e+01 (2.51e+01s)\n",
      "[VAL] iter = 000, acc = 7.90 (1.40e+02s)\n",
      "[TRN] iter = 600, cost = 3.92e+00, |grad| = 1.42e+00 (1.18e+01s)\n",
      "[VAL] iter = 600, acc = 96.64 (1.13e+02s)\n",
      "[TRN] iter = 1200, cost = 3.62e+00, |grad| = 1.30e+00 (1.17e+01s)\n",
      "[VAL] iter = 1200, acc = 97.76 (1.19e+02s)\n",
      "[TRN] iter = 1800, cost = 3.48e+00, |grad| = 1.59e+00 (1.21e+01s)\n",
      "[VAL] iter = 1800, acc = 97.86 (1.17e+02s)\n",
      "[TRN] iter = 2400, cost = 3.23e+00, |grad| = 7.78e-01 (7.67e+00s)\n",
      "[VAL] iter = 2400, acc = 98.36 (7.39e+01s)\n",
      "[TRN] iter = 3000, cost = 3.16e+00, |grad| = 1.15e+00 (5.44e+00s)\n",
      "[VAL] iter = 3000, acc = 98.22 (4.05e+01s)\n",
      "[TRN] iter = 3600, cost = 2.95e+00, |grad| = 1.15e+00 (5.45e+00s)\n",
      "[VAL] iter = 3600, acc = 98.48 (4.08e+01s)\n",
      "[TRN] iter = 4200, cost = 2.84e+00, |grad| = 1.58e+00 (5.42e+00s)\n",
      "[VAL] iter = 4200, acc = 98.52 (4.02e+01s)\n",
      "[TRN] iter = 4800, cost = 2.74e+00, |grad| = 1.26e+00 (5.48e+00s)\n",
      "[VAL] iter = 4800, acc = 98.76 (4.10e+01s)\n",
      "[TRN] iter = 5400, cost = 2.66e+00, |grad| = 2.15e+00 (5.43e+00s)\n",
      "[VAL] iter = 5400, acc = 98.66 (4.05e+01s)\n",
      "[TRN] iter = 6000, cost = 2.56e+00, |grad| = 1.21e+00 (5.53e+00s)\n",
      "[VAL] iter = 6000, acc = 98.82 (4.06e+01s)\n",
      "[TRN] iter = 6600, cost = 2.43e+00, |grad| = 8.62e-01 (5.64e+00s)\n",
      "[VAL] iter = 6600, acc = 98.90 (4.10e+01s)\n",
      "[TRN] iter = 7200, cost = 2.35e+00, |grad| = 9.36e-01 (5.52e+00s)\n",
      "[VAL] iter = 7200, acc = 98.88 (4.12e+01s)\n",
      "[TRN] iter = 7800, cost = 2.28e+00, |grad| = 8.89e-01 (5.43e+00s)\n",
      "[VAL] iter = 7800, acc = 98.96 (4.03e+01s)\n",
      "[TRN] iter = 8400, cost = 2.19e+00, |grad| = 9.66e-02 (5.54e+00s)\n",
      "[VAL] iter = 8400, acc = 98.84 (4.04e+01s)\n",
      "[TRN] iter = 9000, cost = 2.19e+00, |grad| = 1.62e+00 (5.46e+00s)\n",
      "[VAL] iter = 9000, acc = 99.00 (4.03e+01s)\n",
      "[TRN] iter = 9600, cost = 2.09e+00, |grad| = 6.56e-01 (5.39e+00s)\n",
      "[VAL] iter = 9600, acc = 98.94 (4.10e+01s)\n",
      "[TRN] iter = 10200, cost = 2.04e+00, |grad| = 7.21e-01 (5.43e+00s)\n",
      "[VAL] iter = 10200, acc = 99.06 (4.09e+01s)\n",
      "[TRN] iter = 10800, cost = 1.99e+00, |grad| = 4.56e-01 (5.42e+00s)\n",
      "[VAL] iter = 10800, acc = 98.94 (4.06e+01s)\n",
      "[TRN] iter = 11400, cost = 1.95e+00, |grad| = 5.67e-01 (5.41e+00s)\n",
      "[VAL] iter = 11400, acc = 98.92 (4.05e+01s)\n",
      "[TRN] iter = 12000, cost = 1.96e+00, |grad| = 1.53e+00 (5.39e+00s)\n",
      "[VAL] iter = 12000, acc = 98.88 (4.05e+01s)\n",
      "[TRN] iter = 12600, cost = 1.87e+00, |grad| = 3.49e-01 (5.43e+00s)\n",
      "[VAL] iter = 12600, acc = 99.02 (4.04e+01s)\n",
      "[TRN] iter = 13200, cost = 1.86e+00, |grad| = 1.11e+00 (5.46e+00s)\n",
      "[VAL] iter = 13200, acc = 99.10 (4.03e+01s)\n",
      "[TRN] iter = 13800, cost = 1.81e+00, |grad| = 5.52e-01 (5.39e+00s)\n",
      "[VAL] iter = 13800, acc = 99.02 (4.04e+01s)\n",
      "[TRN] iter = 14400, cost = 1.83e+00, |grad| = 1.72e+00 (5.40e+00s)\n",
      "[VAL] iter = 14400, acc = 98.92 (4.04e+01s)\n",
      "[TRN] iter = 15000, cost = 1.75e+00, |grad| = 1.00e+00 (5.41e+00s)\n",
      "[VAL] iter = 15000, acc = 99.06 (4.06e+01s)\n",
      "[TRN] iter = 15600, cost = 1.72e+00, |grad| = 4.40e-01 (5.38e+00s)\n",
      "[VAL] iter = 15600, acc = 98.98 (4.05e+01s)\n",
      "[TRN] iter = 16200, cost = 1.71e+00, |grad| = 1.16e+00 (5.39e+00s)\n",
      "[VAL] iter = 16200, acc = 98.94 (4.03e+01s)\n",
      "[TRN] iter = 16800, cost = 1.67e+00, |grad| = 2.21e-01 (5.46e+00s)\n",
      "[VAL] iter = 16800, acc = 99.04 (4.05e+01s)\n",
      "[TRN] iter = 17400, cost = 1.66e+00, |grad| = 4.44e-01 (5.42e+00s)\n",
      "[VAL] iter = 17400, acc = 99.06 (4.04e+01s)\n",
      "[TRN] iter = 18000, cost = 1.64e+00, |grad| = 9.27e-01 (5.38e+00s)\n",
      "[VAL] iter = 18000, acc = 99.06 (4.03e+01s)\n",
      "[TRN] iter = 18600, cost = 1.62e+00, |grad| = 3.45e-01 (5.45e+00s)\n",
      "[VAL] iter = 18600, acc = 99.00 (4.05e+01s)\n",
      "[TRN] iter = 19200, cost = 1.65e+00, |grad| = 1.83e+00 (5.40e+00s)\n",
      "[VAL] iter = 19200, acc = 99.10 (4.04e+01s)\n",
      "[TRN] iter = 19800, cost = 1.59e+00, |grad| = 7.23e-01 (5.39e+00s)\n",
      "[VAL] iter = 19800, acc = 99.00 (4.02e+01s)\n",
      "[TRN] iter = 20400, cost = 1.57e+00, |grad| = 8.38e-01 (5.38e+00s)\n",
      "[VAL] iter = 20400, acc = 99.12 (4.04e+01s)\n"
     ]
    }
   ],
   "source": [
    "#training and validation\n",
    "indices = collections.deque() # queue containing a permutation of the training indexes\n",
    "for k in range(num_iter, num_total_iter_training):\n",
    "\n",
    "    #Construction of the training batch\n",
    "    if len(indices) < batch_size: # Be sure to have used all the samples before using one a second time.\n",
    "        indices.extend(np.random.permutation(train_data.shape[0])) #reinitialize the queue of indices\n",
    "    idx = [indices.popleft() for i in range(batch_size)] #extract the current batch of samples\n",
    "\n",
    "    #data extraction\n",
    "    batch_data, batch_labels = train_data[idx,:], train_labels[idx] \n",
    "\n",
    "    feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                 learning_obj.ph_labels: batch_labels, \n",
    "                 learning_obj.ph_dropout: dropout_training}\n",
    "\n",
    "    #Training\n",
    "    tic = time.time()\n",
    "    _, current_training_loss, norm_grad = learning_obj.session.run([learning_obj.op_gradients, \n",
    "                                                                    learning_obj.loss, \n",
    "                                                                    learning_obj.norm_grad], feed_dict = feed_dict) \n",
    "    training_time = time.time() - tic\n",
    "\n",
    "    list_training_loss.append(current_training_loss)\n",
    "    list_training_norm_grad.append(norm_grad)\n",
    "\n",
    "    if (np.mod(num_iter, num_iter_val)==0): #validation\n",
    "        msg = \"[TRN] iter = %03i, cost = %3.2e, |grad| = %.2e (%3.2es)\" \\\n",
    "                    % (num_iter, list_training_loss[-1], list_training_norm_grad[-1], training_time)\n",
    "        print(msg)\n",
    "\n",
    "        #Validation Code\n",
    "        tic = time.time()\n",
    "        val_accuracy = 0\n",
    "        for begin in range(0, val_data.shape[0], batch_size):\n",
    "            end = begin + batch_size\n",
    "            end = min([end, val_data.shape[0]])\n",
    "\n",
    "            #data extraction\n",
    "            batch_data = np.zeros((end-begin, val_data.shape[1]))\n",
    "            batch_data = val_data[begin:end,:]\n",
    "            batch_labels = np.zeros(batch_size)\n",
    "            batch_labels[:end-begin] = val_labels[begin:end]\n",
    "\n",
    "            feed_dict = {learning_obj.ph_data: batch_data, \n",
    "                         learning_obj.ph_labels: batch_labels,\n",
    "                         learning_obj.ph_dropout: dropout_val_test}\n",
    "\n",
    "            batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "            val_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "        val_accuracy = val_accuracy/val_data.shape[0]\n",
    "\n",
    "        val_time = time.time() - tic\n",
    "        msg = \"[VAL] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, val_accuracy, val_time)\n",
    "        print(msg)\n",
    "    num_iter += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TST] iter = 21000, acc = 99.04 (8.13e+01s)\n"
     ]
    }
   ],
   "source": [
    "#Test code\n",
    "tic = time.time()\n",
    "test_accuracy = 0\n",
    "for begin in range(0, test_data.shape[0], batch_size):\n",
    "    end = begin + batch_size\n",
    "    end = min([end, test_data.shape[0]])\n",
    "\n",
    "    batch_data = np.zeros((end-begin, test_data.shape[1]))\n",
    "    batch_data = test_data[begin:end,:]\n",
    "\n",
    "    feed_dict = {learning_obj.ph_data: batch_data, learning_obj.ph_dropout: 1}\n",
    "\n",
    "    batch_labels = np.zeros(batch_size)\n",
    "    batch_labels[:end-begin] = test_labels[begin:end]\n",
    "    feed_dict[learning_obj.ph_labels] = batch_labels\n",
    "\n",
    "    batch_accuracy = learning_obj.session.run(learning_obj.accuracy, feed_dict)\n",
    "    test_accuracy += batch_accuracy*batch_data.shape[0]\n",
    "test_accuracy = test_accuracy/test_data.shape[0]\n",
    "test_time = time.time() - tic\n",
    "msg = \"[TST] iter = %03i, acc = %4.2f (%3.2es)\" % (num_iter, test_accuracy, test_time)\n",
    "print(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
